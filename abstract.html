<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Extended Abstract &mdash; cloudmesh-nlp 4.3.2 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="_static/graphviz.css" type="text/css" />
      <link rel="stylesheet" href="_static/copybutton.css" type="text/css" />
      <link rel="stylesheet" href="_static/readthedocs-custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="_static/jquery.js"></script>
        <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/sphinx_highlight.js"></script>
        <script src="_static/clipboard.min.js"></script>
        <script src="_static/copybutton.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="REST" href="rest.html" />
    <link rel="prev" title="Quickstart" href="quickstart.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            cloudmesh-nlp
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">What Is Cloudmesh cc?</a></li>
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="quickstart.html">Quickstart</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Extended Abstract</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#background">Background</a></li>
<li class="toctree-l2"><a class="reference internal" href="#workflow-controlled-computing">Workflow Controlled Computing</a></li>
<li class="toctree-l2"><a class="reference internal" href="#workflow-functionality">Workflow Functionality</a></li>
<li class="toctree-l2"><a class="reference internal" href="#quickstart">Quickstart</a></li>
<li class="toctree-l2"><a class="reference internal" href="#design">Design</a></li>
<li class="toctree-l2"><a class="reference internal" href="#application-demonstration-using-mnist">Application demonstration using MNIST</a></li>
<li class="toctree-l2"><a class="reference internal" href="#summary">Summary</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">User Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="rest.html">REST</a></li>
<li class="toctree-l1"><a class="reference internal" href="rest-gui.html">REST GUI Web Browser Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="python.html">Python Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="specifying-workflows.html">Specifying Workflows</a></li>
<li class="toctree-l1"><a class="reference internal" href="service.html">Service</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="openapi3.html#http://">OpenAPI</a></li>
<li class="toctree-l1"><a class="reference external" href="openapi.json#http://">OpenAPI (json)</a></li>
<li class="toctree-l1"><a class="reference external" href="openapi.yaml#http://">OpenAPI (yaml)</a></li>
<li class="toctree-l1"><a class="reference external" href="py-modindex.html#http://">Python API</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Examples</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="mnist.html">MNIST Workflow</a></li>
<li class="toctree-l1"><a class="reference internal" href="cloudmask.html">Cloudmask Workflow</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Contributors Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="contributing.html">Contributing</a></li>
<li class="toctree-l1"><a class="reference internal" href="todos.html">Todo</a></li>
<li class="toctree-l1"><a class="reference internal" href="acknowledgements.html">Acknowledgments</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">cloudmesh-nlp</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Extended Abstract</li>
      <li class="wy-breadcrumbs-aside">
              <a href="https://github.com/cloudmesh/cloudmesh-nlp/blob/main/docs/source/abstract.rst" class="fa fa-github"> Edit on GitHub</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="extended-abstract">
<h1>Extended Abstract<a class="headerlink" href="#extended-abstract" title="Permalink to this heading"></a></h1>
<p>Hybrid Multi-Cloud Analytics Services Framework and Cloudmesh Controlled Computing through Workflows</p>
<p>Gregor von Laszewski (<a class="reference external" href="mailto:laszewski&#37;&#52;&#48;gmail&#46;com">laszewski<span>&#64;</span>gmail<span>&#46;</span>com</a>)<span class="math notranslate nohighlight">\(^*\)</span>, Jacques
Fleischer</p>
<p><span class="math notranslate nohighlight">\(^*\)</span> Corresponding author</p>
<section id="background">
<h2>Background<a class="headerlink" href="#background" title="Permalink to this heading"></a></h2>
<p>High-performance computing (HPC) is for decades a very important tool
for science. Scientific applications often consist of multiple
tasks/jobs can be leveraging the processing power of requiring
considerable computational needs. Often a supercomputer is needed to
execute the tasks at high speeds while utilizing the specialized
hardware for acceleration that otherwise are not available to the
user. However, these systems can be difficult to use when conducting
analytic programs that leverage machine learning applied to large data
sets to, for example, predict future values or model current
states. For such highly complex analytics tasks, there are often
multiple steps that need to be run repeatedly either to combine
analytics tasks in competition or cooperation to achieve the best
results. Although leveraging computational GPUs lead to several times
higher performance when applied to deep learning algorithms, may be
not possible at the time as the resources are either too expensive or
simply not available. The analytics task is to simplify this dilemma
and introduce a level of abstraction that focuses on the analytics
task while at the same time allowing sophisticated compute resources
to solve the task for the scientist in the background. Hence, the
scientist should be presented with a function call that automatically
puts together the needed resources and stage the task in jobs on the
HPC environment without the need of too many details of the HPC
environment. Instead, the science user should access analytics REST
services that the user can easily integrate into their scientific code
as functions or services. To facilitate the need to coordinate the
many tasks behind such an abstraction we have developed a specialized
analytics Workflow abstraction and service allowing the execution of
multiple analytics tasks in a parallel workflow, The workflow can be
controlled by the user and is asynchronously executed including the
possibility to utilize multiple HPC computing centers via
user-controlled services.</p>
</section>
<section id="workflow-controlled-computing">
<h2>Workflow Controlled Computing<a class="headerlink" href="#workflow-controlled-computing" title="Permalink to this heading"></a></h2>
<p>The Cloudmesh cc Workflow is enhancing Cloudmesh by integrating an API
and service to make using cloud and HPC resources easier. The
enhancement is focused on a library called Cloudmesh Controlled
Computing (cloudmesh-cc) that adds workflow features to control the
execution of tasks and jobs on remote compute resources including
clouds, desktop computers, and batch-controlled HPC with and without
GPUs. Effectively we access remote, and hybrid resources by
integrating cloud, and on-premise resources.</p>
<p>The goal is to provide an easy way to access these resources, while at
the same time providing the ability to integrate the computational
power enabled through a parallel workflow framework Access to these
complex resources is provided through easy to use interfaces such as a
python API, REST services, and command line tools. Through these
interfaces, the framework is universal and can be integrated into the
science application or other higher level frameworks and even
different programming languages.</p>
<p>The software developed is freely available and can easily be installed
with standard python tools so integration in the python ecosystem
using virtualenv’s and anaconda is simple.</p>
</section>
<section id="workflow-functionality">
<h2>Workflow Functionality<a class="headerlink" href="#workflow-functionality" title="Permalink to this heading"></a></h2>
<p>The framework supports workflow functionality to (a) execute workflow
tasks in parallel (b) manage the creation of the workflow by adding
graphs, tasks, and edges (c) control the execution and (d) monitor the
execution The implicit design to access the workflow through an API, a
REST services, and the command line allows easy integration into other
frameworks.</p>
<p>The REST interface is depicted in <a class="reference internal" href="#fastapi-service"><span class="std std-numref">Fig. 2</span></a> and is
also available as OpenAPI document.</p>
<figure class="align-default" id="fastapi-service">
<img alt="Figure OpenAPI Description of the REST Interface to the Workflow" src="images/fastapi-service.png" />
<figcaption>
<p><span class="caption-number">Fig. 2 </span><span class="caption-text">OpenAPI Description of the REST Interface to the Workflow</span><a class="headerlink" href="#fastapi-service" title="Permalink to this image"></a></p>
</figcaption>
</figure>
<p>In addition, the framework supports multiple operating systems like macOS,
Linux, and Windows 10 and 11. This not only includes the ability to run
the workflow on remote computers but also integrates tasks that can be run
locally on the various operating systems to integrate their computational
capabilities. Hence we support easy access to host capabilities, such as
the computer’s localhost, remote computers, and the Linux-based virtual
image WSL. Jobs can be visualized and saved as a YAML and SVG data file.</p>
</section>
<section id="quickstart">
<h2>Quickstart<a class="headerlink" href="#quickstart" title="Permalink to this heading"></a></h2>
<p>To utilize the workflow program, prepare a cm directory in your home
directory by executing the following commands in a terminal:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span>~
mkdir<span class="w"> </span>cm
<span class="nb">cd</span><span class="w"> </span>cm
pip<span class="w"> </span>install<span class="w"> </span>cloudmesh-installer<span class="w"> </span>-U
cloudmesh-installer<span class="w"> </span>get<span class="w"> </span>cc
<span class="nb">cd</span><span class="w"> </span>cloudmesh-cc
pytest<span class="w"> </span>-v<span class="w"> </span>-x<span class="w"> </span>--capture<span class="o">=</span>no<span class="w"> </span>tests/test_131_workflow_local.py
</pre></div>
</div>
<p>This test runs a number of jobs on the local machine
within a singular workflow: the first job runs
a local shell script, the second runs a local Python script, and the
third runs a local Jupyter notebook.</p>
</section>
<section id="design">
<h2>Design<a class="headerlink" href="#design" title="Permalink to this heading"></a></h2>
<p>The hybrid multi-cloud analytics service framework ensures running
jobs across many platforms. The design includes a small and
streamlined number of abstractions so that jobs and workflows can be
represented easily. This makes it possible to custom design for each
target type a specific job type so that execution on local and remote
compute resources including batch operating systems can be achieved.</p>
<p>Job types for localhost, ssh, SLURM, and WSL are available.  Other job
types can easily be added.  The design is flexible and new job can be
expanded as each job can contain arbitrary arguments. Through this
flexibility jobs types can be also run on different operating systems
including local job on Linux, macOS, Windows 10, and Windows 11, jobs
running in WSL on Windows computers.</p>
<p>An important design requirement to display the dependencies of the
workflow in a direct acyclic Graph is enabled by reusing the Networkx
Graph framework.  This greatly reduced the complexity of the
implementation while being able to leverage graphical displays of the
workflow, as well as implementing sequential execution of workflows as
an alternative to parallel execution while using the build-in
topological sort function.  It serves as an example that custom
schedulers can be designed and easily integrated into the runtime
management while executing the tasks and jobs through a
straightforward interface.  The status of the tasks and jobs is stored
in a file database that can be monitored during program execution. The
creation of the jobs is done prior to the execution of the workflow,
but additional tasks and jobs could be integrated also at
runtime. This is possible when using our parallel scheduler that
selects tasks and jobs once the parent jobs have been completed.  This
is important as it allows dynamic workflow execution of long-running
workflows, while results from previous calculations can be used in
later stages of the workflow and lead to workflow modifications.</p>
<p>We have developed a simple-to-use Python API so programs are easy to
write.  Additionally, we used this API internally to implement a REST
service to deliver a language-independent framework. The obvious
functions to manage workflows are supported including graph
specification through configuration files, upload of workflows,
export, adding jobs and dependencies, and visualizing the workflow
during the execution. An important feature that we added is the
monitoring of the jobs while using progress reports through automated
log file mining. This way each job reports the progress during the
execution. This is also especially important when we run very complex
and long-running jobs.</p>
<p>The REST service was implemented in FastAPI to leverage a small but fast
service that features a much smaller footprint for implementation and
setup in contrast to other similar REST service frameworks using python.</p>
<p>The architectural component building this framework is depicted in
<a class="reference internal" href="#workflow-architecture"><span class="std std-numref">Fig. 3</span></a>. The code is available in this
repository and manual pages are provided on how to install it:
<a class="reference external" href="https://github.com/cloudmesh/cloudmesh-cc">cloudmesh-cc</a>.</p>
<p>See <a class="reference internal" href="#workflow-architecture"><span class="std std-numref">Fig. 3</span></a> for a diagram of the workflow
components.</p>
<figure class="align-default" id="workflow-architecture">
<img alt="Figure Design for the workflow." src="images/workflow-architecture.png" />
<figcaption>
<p><span class="caption-number">Fig. 3 </span><span class="caption-text">The architecture of the Cloudmesh cc Workflow</span><a class="headerlink" href="#workflow-architecture" title="Permalink to this image"></a></p>
</figcaption>
</figure>
</section>
<section id="application-demonstration-using-mnist">
<h2>Application demonstration using MNIST<a class="headerlink" href="#application-demonstration-using-mnist" title="Permalink to this heading"></a></h2>
<p>The Modified National Institute of Standards and Technology (MNIST)
Database is large database of handwritten digits. We provide number
example applications using the database. This includes Multilayer
Perceptron, LSTM (Long short-term memory), Auto-Encoder,
Convolutional, and Recurrent Neural Networks, Distributed Training,
and PyTorch training. With Cloudmesh cc we developed a number of
example workflows that run the algorithms on multiple machines.</p>
</section>
<section id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Permalink to this heading"></a></h2>
<p>We have developed a sophisticated but easy-to-use workflow system
allowing tasks and jobs to be executed on on-premise and remote
machines. Various interfaces exist such as an API, REST service, and
command line tool. With the framework, researchers and scientists
should be able to create jobs on their own, place them in the
workflow, and run them on various types of computers. These jobs can
have analytics functions included and themselves be exposed through an
API, REST service, or command line while hiding the complex setup
needed for collaborating and competing workflow needs accessing
distributed resources.  In addition, developers and users can utilize
the built-in OpenAPI graphical user interface to manage workflows
between jobs. They can be uploaded as YAML files or individually added
through the built-in debug framework.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="quickstart.html" class="btn btn-neutral float-left" title="Quickstart" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="rest.html" class="btn btn-neutral float-right" title="REST" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Gregor von Laszewski and Jacques Fleischer.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>